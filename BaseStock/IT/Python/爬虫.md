爬虫作用和原理
	**概念**:互联网是张网,爬虫就是蜘蛛去获取其中的猎物(数据)
	**作用**:
	1. 收集数据
		1. 数据分析
		2. 制作伪装数据
	2. 监控
		1. 热点捕捉
	**原理**:
	伪装成请求去获取页面数据
反爬虫
	**作用**:防止虚拟数据的访问
	**实现方式**:
		1. User-Agent(特殊字符串头)检测,能够使服务器识别客户端的操作系统电脑信息和浏览器等信息
		2. 验证码
		3. ip限制  请求频率过高是判断是否爬虫进行限制
		4. 动态页面技术 网页返回的是js的数据而不是网络真实数据
		5. 加密
爬虫获取数据后解析数据
	
urllib
	**概念**:urllib是Python标准库中的一个模块，用于处理URL（Uniform Resource Locator）的请求。
	**作用**:它提供了一系列用于处理URL的函数和类，包括发起HTTP请求、处理响应、编码和解码URL等功能。可以用于爬取网页、发送HTTP请求、下载文件等等
	**方法**:
		1. request.urlopen(地址)  打开指定网页并返回一个httpresponse对象可被read后decode成指定字节格式
		2. request.urlretrieve(地址,文件名.指定格式)   爬取对应地址的内容保存在本地
		3. parse.quote()SenseTime or Megvi解析成unicode字符

爬虫分类
	**通用网络爬虫**  
		爬行对象从一些种子 URL 扩充到整个 Web. 由于商业原因，技术细节很少公布。 这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。 
		**常用**:门户站点搜索引擎和大型 Web 服务提供商采集数据
		**缺点**:大多数数据是无用的,且无法根据用户需求精准获取数据
		**结构大致分类**
			1. 页面爬行模块 、
			2. 页面分析模块、
			3. 链接过滤模块、
			4. 页面数据库、
			5. URL 队列、
			6. 初始 URL 
		**常用的爬行策略**
			1. **深度优先策略**：其基本方法是按照深度由低到高的顺序，依次访问下一级网页链接，直到不能再深入为止。 爬虫在完成一个爬行分支后返回到上一链接节点进一步搜索其它链接。 当所有链接遍历完后，爬行任务结束。 这种策略比较适合垂直搜索或站内搜索， 但爬行页面内容层次较深的站点时会造成资源的巨大浪费 。  
			2. **广度优先策略**：此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。 当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。 这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需较长时间才能爬行到目录层次较深的页面 。  
	**聚焦网络爬虫  (常用)**
		是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫。聚焦爬虫只需要爬行与主题相关的页面，极大地节省了硬件和网络资源
		聚焦网络爬虫和通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚焦爬虫爬行策略实现的关键是评价页面内容和链接的重要性
		**特点**:指定URL或指定主题进行爬取,节省空间
		**爬行策略:**
			1.**基于内容评价的爬行策略**：DeBra将文本相似度的计算方法引入到网络爬虫中，提出了 Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关 度 的 高 低 。 Herseovic对 Fish Search 算 法 进 行 了 改 进 ，提 出 了 Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小 。  
			2.**基于链接结构评价的爬行策略** ：Web 页面作为一种半结构化文档，包含很多结构信息，可用来评价链接重要性。 PageRank 算法最初用于搜索引擎信息检索中对查询结果进行排序，也可用于评价链接重要性，具体做法就是每次选择 PageRank 值较大页面中的链接来访问。 另一个利用 Web结构评价链接价值的方法是 HITS 方法，它通过计算每个已访问页面的 Authority 权重和 Hub 权重，并以此决定链接的访问顺序 。  
			3.**基于增强学习的爬行策略**：Rennie 和 McCallum 将增强学习引入聚焦爬虫，利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序。  
			4.**基于语境图的爬行策略**：Diligenti 等人提出了一种通过建立语境图（Context Graphs）学习网页之间的相关度，训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离越近的页面中的链接优先访问。印度理工大学（IIT）和 IBM 研究中心的研究人员开发了一个典型的聚焦网络爬虫。 该爬虫对主题的定义既不是采用关键词也不是加权矢量，而是一组具有相同主题的网页。 它包含两个重要模块：一个是分类器，用来计算所爬行的页面与主题的相关度，确定是否与主题相关；另一个是净化器，用来识别通过较少链接连接到大量相关页面的中心页面。  
	**增量式网络爬虫**  
		增量式网络爬虫（Incremental Web Crawler）是 指 对 已 下 载 网 页 采 取 增 量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。 和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面 ，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。增量式网络爬虫的体系结构\[包含爬行模块、排序模块、更新模块、本地页面集、待爬行 URL 集以及本地页面URL 集 。  
		**常用**:需要实时更新数据的应用程序，例如股票行情、天气预报等。
		增量式爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面的质量。 为实现第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容，常用的方法有：1.统一更新法：爬虫以相同的频率访问所有网页，不考虑网页的改变频率；2.个体更新法：爬虫根据个体网页的改变频率来重新访问各页面；3) 基于分类的更新法：爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问这两类网页。  
		为实现第二个目标，增量式爬虫需要对网页的重要性排序，常用的策略有：广度优先策略、PageRank 优先策略等。IBM 开发的 WebFountain是一个功能强大的增量式网络爬虫，它采用一个优化模型控制爬行过程，并没有对页面变化过程做任何统计假设，而是采用一种自适应的方法根据先前爬行周期里爬行结果和网页实际变化速度对页面更新频率进行调整。北京大学的天网增量爬行系统旨在爬行国内 Web，将网页分为变化网页和新网页两类，分别采用不同爬行策略。 为缓解对大量网页变化历史维护导致的性能瓶颈，它根据网页变化时间局部性规律，在短时期内直接爬行多次变化的网页 ，为尽快获取新网页，它利用索引型网页跟踪新出现网页。  
	**深度网络Deep Web 爬虫**  
		Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。 表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。**Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面**。例如那些用户注册后内容才可见的网页就属于 Deep Web。Deep Web 中可访问信息容量是 Surface Web 的百倍，是互联网上最大、发展最快的新型信息资源 。  
		Deep Web 爬虫体系结构包含六个基本功能模块 （爬行控制器、解析器、表单分析器、表单处理器、响应分析器、LVS 控制器）和两个爬虫内部数据结构（URL 列表、LVS 表）。 其中 LVS（Label Value Set）表示标签/数值集合，用来表示填充表单的数据源。  
		Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：  
		1.基于领域知识的表单填写：此方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。 Yiyao Lu[25]等人提出一种获取 Form 表单信息的多注解方法，将数据表单按语义分配到各个组中 ，对每组从多方面注解，结合各种注解结果来预测一个最终的注解标签；利用一个预定义的领域本体知识库来识别 Deep Web 页面内容， 同时利用一些来自 Web 站点导航模式来识别自动填写表单时所需进行的路径导航。  
		2.基于网页结构分析的表单填写： 此方法一般无领域知识或仅有有限的领域知识，将网页表单表示成 DOM 树，从中提取表单各字段值。 Desouky 等人提出一种 LEHW 方法，该方法将 HTML 网页表示为DOM 树形式，将表单区分为单属性表单和多属性表单，分别进行处理；孙彬等人提出一种基于 XQuery 的搜索系统，它能够模拟表单和特殊页面标记切换，把网页关键字切换信息描述为三元组单元，按照一定规则排除无效表单，将 Web 文档构造成 DOM 树，利用 XQuery 将文字属性映射到表单字段 。
		Raghavan 等人提出的 HIWE 系统中，爬行管理器负责管理整个爬行过程，分析下载的页面，将包含表单的页面提交表单处理器处理，表单处理器先从页面中提取表单，从预先准备好的数据集中选择数据自动填充并提交表单，由爬行控制器下载相应的结果页面 。
高匿名,匿名和透明代理
	**高匿名**:服务器不知道你使用了代理.也不知道你的真实ip
	**匿名**:服务器知道你使用了代理,但不知道你的真实ip
	**透明代理**:服务器知道你使用了代理,也知道你的真实ip


